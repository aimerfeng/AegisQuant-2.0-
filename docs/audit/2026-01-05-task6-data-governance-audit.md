# Task 6 æ•°æ®æ²»ç†ä¸­å¿ƒæ¨¡å— - æ¶æ„å®¡è®¡æŠ¥å‘Š

**å®¡è®¡æ—¥æœŸ**: 2026-01-05  
**å®¡è®¡çŠ¶æ€**: âœ… é€šè¿‡  
**æ¨¡å—**: core/data/ (importer.py, cleaner.py, storage.py)

---

## 1. Data Pipeline Architecture

### âœ… 1.1 Storage Strategy

The decision to use Parquet with Hive-style partitioning (exchange/symbol/period) is excellent. This layout minimizes I/O overhead for strategy backtesting, where queries almost always filter by symbol and date range.

**å­˜å‚¨è·¯å¾„è®¾è®¡**:
- Tick æ•°æ®: `database/ticks/{exchange}/{symbol}/{date}.parquet`
- Bar æ•°æ®: `database/bars/{exchange}/{symbol}/{interval}.parquet`

**ä¼˜åŠ¿**:
- æŒ‰äº¤æ˜“æ‰€/åˆçº¦/å‘¨æœŸåˆ†åŒºï¼Œå‡å°‘å›æµ‹æ—¶çš„ I/O å¼€é”€
- æ”¯æŒ snappy å‹ç¼©ï¼Œå¹³è¡¡å‹ç¼©ç‡å’Œè¯»å–é€Ÿåº¦
- ç¬¦åˆé‡åŒ–å›æµ‹çš„å…¸å‹æŸ¥è¯¢æ¨¡å¼

### âœ… 1.2 Data Integrity

The DataCleaner module correctly addresses the "Garbage In, Garbage Out" problem. The implementation of forward-fill (ffill) for missing prices is standard financial practice. Time alignment logic ensures bars start exactly on the minute/hour mark.

**æ•°æ®æ¸…æ´—ç­–ç•¥**:
- Forward Fill (ffill): é‡‘èæ•°æ®æ ‡å‡†åšæ³•ï¼Œç”¨å‰å€¼å¡«å……ç¼ºå¤±ä»·æ ¼
- Linear Interpolation: é€‚ç”¨äºè¿ç»­æ•°æ®çš„çº¿æ€§æ’å€¼
- 3Ïƒ å¼‚å¸¸å€¼æ£€æµ‹: åŸºäºç»Ÿè®¡å­¦çš„å¼‚å¸¸å€¼è¯†åˆ«
- æ—¶é—´æˆ³å¯¹é½: ç¡®ä¿ K çº¿ç²¾ç¡®å¯¹é½åˆ°åˆ†é’Ÿ/å°æ—¶è¾¹ç•Œ

---

## 2. Code Quality & Robustness

### âœ… 2.1 Importer Abstraction

`core/data/importer.py` provides a unified interface for loading data, abstracting away the differences between CSV, Excel, and Parquet. Error handling for malformed files is present and throws typed `DataError` exceptions.

**è®¾è®¡äº®ç‚¹**:
- ç»Ÿä¸€çš„æ•°æ®åŠ è½½æ¥å£ï¼Œå±è”½æ ¼å¼å·®å¼‚
- åŸºäºæ‰©å±•åå’Œæ–‡ä»¶å¤´çš„æ™ºèƒ½æ ¼å¼è¯†åˆ«
- ç±»å‹åŒ–å¼‚å¸¸å¤„ç† (`DataError`)

### ğŸŸ¡ 2.2 Scalability Note

**Location**: `core/data/importer.py`

**Observation**: Loading entire large files into memory via Pandas.

**Recommendation**: Acceptable for datasets < 10GB. For future scalability (Task 34+), consider integrating Polars or Dask for lazy evaluation and out-of-core processing.

**å½“å‰çŠ¶æ€**: 
- ä½¿ç”¨ Pandas åŠ è½½æ•°æ®ï¼Œé€‚ç”¨äº < 10GB æ•°æ®é›†
- Polars åœ¨ Windows ç¯å¢ƒå®‰è£…å¤±è´¥ï¼Œæš‚æ—¶ä½¿ç”¨ Pandas æ›¿ä»£

**æœªæ¥ä¼˜åŒ– (v2.0)**:
- é›†æˆ Polars å®ç°æƒ°æ€§æ±‚å€¼
- è€ƒè™‘ Dask æ”¯æŒè¶…å¤§æ•°æ®é›†çš„åˆ†å¸ƒå¼å¤„ç†
- å®ç°æµå¼æ•°æ®åŠ è½½ï¼Œé¿å…å†…å­˜æº¢å‡º

---

## 3. Test Coverage

### âœ… 3.1 Property Tests

`tests/test_data_governance.py` validates critical properties:

| Property | Description | Status |
|----------|-------------|--------|
| **Idempotency** | Cleaning already clean data does not change it | âœ… PASSED |
| **No Data Loss** | Cleaning does not drop rows unless explicitly configured to remove outliers | âœ… PASSED |
| **Schema Consistency** | Output columns always match the expected OHLCV format | âœ… PASSED |

**å±æ€§æµ‹è¯•è¦†ç›–**:
- Property 3: Data Format Detection âœ“
- Property 4: Missing Value Fill Correctness âœ“
- Property 5: Outlier Detection Accuracy âœ“
- Property 6: Timestamp Alignment Validation âœ“
- Property 7: Data Persistence Round-Trip âœ“

---

## 4. å®¡è®¡ç»“è®º

### é€šè¿‡é¡¹ âœ…
1. Parquet + Hive åˆ†åŒºå­˜å‚¨ç­–ç•¥è®¾è®¡ä¼˜ç§€
2. æ•°æ®æ¸…æ´—é€»è¾‘ç¬¦åˆé‡‘èè¡Œä¸šæ ‡å‡†
3. ç»Ÿä¸€çš„å¯¼å…¥æ¥å£æŠ½è±¡è‰¯å¥½
4. å¼‚å¸¸å¤„ç†ä½¿ç”¨ç±»å‹åŒ–å¼‚å¸¸
5. å±æ€§æµ‹è¯•è¦†ç›–å…³é”®æ­£ç¡®æ€§å±æ€§

### å¾…ä¼˜åŒ–é¡¹ ğŸŸ¡
1. **å¯æ‰©å±•æ€§**: å½“å‰ Pandas å®ç°é€‚ç”¨äºä¸­ç­‰è§„æ¨¡æ•°æ®ï¼Œå¤§è§„æ¨¡æ•°æ®éœ€è€ƒè™‘ Polars/Dask

### è¡ŒåŠ¨é¡¹
- [ ] (v2.0) è¯„ä¼° Polars åœ¨ Windows ç¯å¢ƒçš„å®‰è£…é—®é¢˜
- [ ] (v2.0) å®ç°æµå¼æ•°æ®åŠ è½½æ¥å£
- [ ] (Task 34+) å‰ç«¯æ•°æ®ä¸­å¿ƒç»„ä»¶é›†æˆæ—¶è€ƒè™‘åˆ†é¡µåŠ è½½

---

**å®¡è®¡äºº**: Architecture Review Bot  
**ä¸‹ä¸€æ­¥**: ç»§ç»­ Task 7 æ•°æ®æºæ’ä»¶æ¨¡å—
